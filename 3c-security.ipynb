{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DDoS and Intrusion detection\n",
    "\n",
    "\n",
    "In this exercise, we'll use Spark structured streaming to detect DDoS attacks and attempts to access the admin panel of the website.\n",
    "\n",
    "First, we'll add the same test_query function function from the cleanup notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from time import sleep\n",
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.2.0 pyspark-shell'\n",
    "\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "import pyspark \n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "\n",
    "def test_query(sdf, mode=\"append\", rows=None, wait=2, sort=None):\n",
    "    # If a query with the same name exists, stop it.\n",
    "    query_name = \"test_query\"\n",
    "    query = None\n",
    "    for q in spark.streams.active:\n",
    "        if (q.name == query_name):\n",
    "            query = q\n",
    "    if query is not None:\n",
    "        query.stop()\n",
    "\n",
    "    try:\n",
    "        tq = (\n",
    "            # Create an output stream\n",
    "            sdf.writeStream               \n",
    "            # Only write new rows to the output\n",
    "            .outputMode(mode)           \n",
    "            # Write output stream to an in-memory Spark table (a DataFrame)\n",
    "            .format(\"memory\")               \n",
    "            # The name of the output table will be the same as the name of the query\n",
    "            .queryName(query_name)\n",
    "            # Submit the query to Spark and execute it\n",
    "            .start()\n",
    "        )\n",
    "\n",
    "        tq.processAllAvailable()\n",
    "\n",
    "        sleep(wait)\n",
    "        while(tq.status.get(\"isTriggerActive\") == True):\n",
    "            print(f\"DataAvailable: {tq.status['isDataAvailable']},\\tTriggerActive: {tq.status['isTriggerActive']}\\t{tq.status['message']}\")\n",
    "            sleep(wait)\n",
    "\n",
    "        # When the status says \"Waiting for data to arrive\", that means the query\n",
    "        # has finished its current iteration and is waiting for new messages from\n",
    "        # Kafka.\n",
    "        print(f\"DataAvailable: {tq.status['isDataAvailable']},\\tTriggerActive: {tq.status['isTriggerActive']}\\t{tq.status['message']}\")\n",
    "\n",
    "        memory_sink = spark.table(query_name)\n",
    "\n",
    "        if sort:\n",
    "            memory_sink = memory_sink.sort(*sort)\n",
    "\n",
    "        # Show result table in Jupyter Notebook. Since Jupyter Notebooks have native support for showing pandas tables,\n",
    "        # we convert the Spark DataFrame.\n",
    "        if rows:\n",
    "            display(memory_sink)\n",
    "            display(memory_sink.take(10))\n",
    "        else:\n",
    "            display(memory_sink)\n",
    "            display(memory_sink.toPandas())\n",
    "\n",
    "    finally:\n",
    "        # Always try to stop the query but it doesn't matter if it fails.\n",
    "        try:\n",
    "            tq.stop()\n",
    "        except:\n",
    "            pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Install the required Python 3 dependencies\n",
    "python3 -m pip install kafka-python pyarrow  # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a Spark context and specify that the python spark-kafka libraries need to be added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a local Spark cluster with two executors (if it doesn't already exist)\n",
    "spark = SparkSession.builder.master('local[2]').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a streaming DataFrame that represents the events received from the Kafka topic `clicks-cleaned`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = (\n",
    "    \n",
    ")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cast the json to columns in the DataFrame. Make sure to use TimestampType for the `ts_ingest` since we already converted it in the `clean` notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_json_stream = (\n",
    "    \n",
    ")\n",
    "\n",
    "test_query(decoded_json_stream, mode=\"append\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a [user-defined function (`udf`)](https://docs.databricks.com/spark/latest/spark-sql/udf-python.html) `forbidden_clicks` which takes a URL as input and returns `True` if the URL points to the admin part of the website (when it ends with `/admin`).\n",
    "\n",
    "As an example, the following code creates a UDF which squares each value of a column. It is used on the \"id\" column and the resulting column's name is changed to \"id_squared\".\n",
    "\n",
    "```python\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "@udf(\"long\")\n",
    "def squared_udf(s):\n",
    "  return s * s\n",
    "\n",
    "df = spark.table(\"test\")\n",
    "display(df.select(\"id\", squared_udf(\"id\").alias(\"id_squared\")))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the UDF to create the dataframe `df_forbidden` which contains the collumn `forbidden` which specifies if the URL is an admin URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_forbidden = (\n",
    "    \n",
    ")\n",
    "\n",
    "test_query(df_forbidden, mode=\"append\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll do the same for detecting ddos attacks. First we want to flag whether an individual event is suspicious, i.e. whether the page_timer and page_height are both `0`. However, this time we'll use a `pandas_udf`.\n",
    "\n",
    "[Regular Python UDF's have the disadvantage that they operate on one row at a time](https://databricks.com/blog/2017/10/30/introducing-vectorized-udfs-for-pyspark.html), causing them to suffer from high serialization and invocation overhead. Pandas UDF's are built on top of Apache Arrow to support high-performant UDF's in Python.\n",
    "\n",
    "This is the squared_udf converted to a pandas udf.\n",
    "\n",
    "```python\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "\n",
    "@pandas_udf('long', PandasUDFType.SCALAR)\n",
    "def squared_pandas_udf(s):\n",
    "    return s * s\n",
    "\n",
    "df = spark.table(\"test\")\n",
    "display(df.select(\"id\", squared_udf(\"id\").alias(\"id_squared\")))\n",
    "```\n",
    "\n",
    "The regular UDF version works one row at a time: the user-defined function takes a long `s` and returns the result of `s*s` as a long. In the Pandas version, the user-defined function takes a pandas.Series `s` and returns the result of `s*s` as a pandas.Series. Because `s*s` is vectorized on `pandas.Series`, the Pandas version is much faster than the row-at-a-time version.\n",
    "\n",
    "Note that there are two important requirements when using scalar pandas UDFs:\n",
    "\n",
    "* The input and output series must have the same size.\n",
    "* How a column is split into multiple pandas.Series is internal to Spark, and therefore the result of user-defined function must be independent of the splitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ddos = (\n",
    "\n",
    ")\n",
    "\n",
    "test_query(df_ddos, mode=\"append\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell above we highlight the use of high performance User Defined Functions (UDF's) with pandas. For simple use cases such as the one here we could also avoid using UDF's and write the following instead:\n",
    "\n",
    "```python\n",
    "df_ddos = df_data.withColumn('flagged', when((col('visitor_page_timer') == 0) & (col('visitor_page_height') == 0), True).otherwise(False))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second step in detecting a ddos attack is counting how many suspicious events happen within a certain timeframe. For this, well combine `groupBy` and a 30 seconds `window` based on the `ts_ingest` timestamp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ddos_window = (\n",
    "    \n",
    ")\n",
    "\n",
    "test_query(df_ddos_window, mode=\"complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that for this query, we're using outputmode `complete` instead of `append`. This is because `append` mode can never change a row in the result table once it's written. However, Spark does not know when all events of a certain window have been seen. Spark assumes by default that data can be \"late\", meaning an earlier event can enter the stream _after_ a later event has entered. In `complete` mode, rows are written to the result table immediately when they become available and they are updated once new data arrives.\n",
    "\n",
    "Although this solution in complete mode works, it will consume a lot of RAM over time because all intermediary results for all windows will be saved. Even if those windows ended years ago!\n",
    "\n",
    "In order to solve this memory issue, you need to define when Spark can assume that it will not receive events from a certain window anymore. This is done using a [`watermark`](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#handling-late-data-and-watermarking). With a watermark, you specify how \"late\" data can be.\n",
    "\n",
    "For this exercise, you can assume data will not arrive more than 10 seconds late.\n",
    "\n",
    "* Use [`withWatermark`](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.withWatermark.html) to add a watermark to the query with a treshold of 10 seconds. Using the timestamp as eventime.\n",
    "* Run the query using append mode.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ddos_window_watermark = (\n",
    "    \n",
    ")\n",
    "\n",
    "test_query(df_ddos_window_watermark, mode=\"append\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now run these queries and write the output to `clicks-calculated-forbidden` and `clicks-calculated-ddos`. Use a trigger with `processingTime = \"30 seconds\"` for the ddos query so that the next interval is only calculated 30 seconds after the first interval starts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Remove old checkpoint dir\n",
    "    os.rmdir(\"checkpoints-forbidden\")\n",
    "except FileNotFoundError as e:\n",
    "    pass\n",
    "except OSError as e:\n",
    "    # Ignore \"file not found\" errors\n",
    "    if (e.errno != 39):\n",
    "        raise e\n",
    "\n",
    "query_forbidden = (\n",
    "    \n",
    ")\n",
    "\n",
    "# Sleep two seconds\n",
    "sleep(2)\n",
    "\n",
    "# Show the status of the query\n",
    "display(query_forbidden.status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Remove old checkpoint dir\n",
    "    os.rmdir(\"checkpoints-ddos\")\n",
    "except FileNotFoundError as e:\n",
    "    pass\n",
    "except OSError as e:\n",
    "    # Ignore \"file not found\" errors\n",
    "    if (e.errno != 39):\n",
    "        raise e\n",
    "\n",
    "query_ddos = (\n",
    "    \n",
    ")\n",
    "\n",
    "# Sleep two seconds\n",
    "sleep(2)\n",
    "\n",
    "# Show the status of the query\n",
    "display(query_ddos.status)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark helpers\n",
    "\n",
    "The following code stops all running queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sleep(2)\n",
    "\n",
    "for q in spark.streams.active:\n",
    "    print(\"Stopping query '{}' with name '{}'\".format(q.id, q.name))\n",
    "    q.stop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
